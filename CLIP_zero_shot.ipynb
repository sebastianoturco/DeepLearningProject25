{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYQiprE0pcVD"
      },
      "source": [
        "# CLIP zero-shot Evaluation\n",
        "This short notebook implements the dataset split into base and novel categories (see project assignment) and runs the zero-shot evaluation with CLIP.\n",
        "Feel free to copy the code contained in this notebook or to directly use this notebook as starting point for you project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzXtFjhh7iOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e935ebf-7e2b-402f-f180-b74313fb9a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai_clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from openai_clip)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai_clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai_clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai_clip) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai_clip\n",
            "  Building wheel for openai_clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai_clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=97a6a95f5c8ece85876362cbbbd069fb1c41fbc08ab98b0f58be2aef3de3355f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\n",
            "Successfully built openai_clip\n",
            "Installing collected packages: ftfy, openai_clip\n",
            "Successfully installed ftfy-6.3.1 openai_clip-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# we need to install clip as it is not pre-installed\n",
        "# you are also free to use open_clip which provide more models\n",
        "# https://github.com/mlfoundations/open_clip\n",
        "%pip install openai_clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtqdSOr8qqOn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import clip\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2353MHw1p24h"
      },
      "source": [
        "## Dataset Loading\n",
        "Let's get the data directly from torchvision as we have seen during labs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_1CrUhZpVCq"
      },
      "outputs": [],
      "source": [
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\n",
        "    Args:\n",
        "        data_dir (str): Directory where the dataset will be stored.\n",
        "        transform (torch.Compose)\n",
        "    Returns:\n",
        "        tuple: A tuple containing the train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    return train, val, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJI_a5EizA5a"
      },
      "source": [
        "## Base and Novel categories\n",
        "To split in base and novel categories we list all dataset classes, and count their number (we already know it's 102 but let's do it properly).\n",
        "Then, we just allocate the first half to base categories and the remaining half to novel ones.\n",
        "We can do this because we are simulating a real world application, but keep in mind this will not happen out there!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfq51vd8q_5a"
      },
      "outputs": [],
      "source": [
        "def base_novel_categories(dataset):\n",
        "    # set returns the unique set of all dataset classes\n",
        "    all_classes = set(dataset._labels)\n",
        "    # and let's count them\n",
        "    num_classes = len(all_classes)\n",
        "\n",
        "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
        "    # then we slice the list in half and generate base and novel category lists\n",
        "    base_classes = list(range(num_classes))[:num_classes//2]\n",
        "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
        "    return base_classes, novel_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvDdoYQr2fIu"
      },
      "source": [
        "## Inspect Classes\n",
        "Let's now visualize which are the base and novel classes.\n",
        "To do so, we first get a dummy test set (without augmentations) as we are just interested in the dataset labels. Then, we split it useing `base_novel_categories`.\n",
        "Finally, we use the hard-coded CLASS_NAMES to print the class in natural language.\n",
        "\n",
        "> Note: the list of class names was only recently added to `torchvision.datasets.Flowers102`. To avoid useless errors that can occour to you, we decided to also provide such a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veGGpNDctCgR",
        "outputId": "a1aedd58-f4a6-48e1-a46e-01a7f63db28a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 345M/345M [00:20<00:00, 17.1MB/s]\n",
            "100%|██████████| 502/502 [00:00<00:00, 947kB/s]\n",
            "100%|██████████| 15.0k/15.0k [00:00<00:00, 29.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Class Names: [(0, 'pink primrose'), (1, 'hard-leaved pocket orchid'), (2, 'canterbury bells'), (3, 'sweet pea'), (4, 'english marigold'), (5, 'tiger lily'), (6, 'moon orchid'), (7, 'bird of paradise'), (8, 'monkshood'), (9, 'globe thistle'), (10, 'snapdragon'), (11, \"colt's foot\"), (12, 'king protea'), (13, 'spear thistle'), (14, 'yellow iris'), (15, 'globe-flower'), (16, 'purple coneflower'), (17, 'peruvian lily'), (18, 'balloon flower'), (19, 'giant white arum lily'), (20, 'fire lily'), (21, 'pincushion flower'), (22, 'fritillary'), (23, 'red ginger'), (24, 'grape hyacinth'), (25, 'corn poppy'), (26, 'prince of wales feathers'), (27, 'stemless gentian'), (28, 'artichoke'), (29, 'sweet william'), (30, 'carnation'), (31, 'garden phlox'), (32, 'love in the mist'), (33, 'mexican aster'), (34, 'alpine sea holly'), (35, 'ruby-lipped cattleya'), (36, 'cape flower'), (37, 'great masterwort'), (38, 'siam tulip'), (39, 'lenten rose'), (40, 'barbeton daisy'), (41, 'daffodil'), (42, 'sword lily'), (43, 'poinsettia'), (44, 'bolero deep blue'), (45, 'wallflower'), (46, 'marigold'), (47, 'buttercup'), (48, 'oxeye daisy'), (49, 'common dandelion'), (50, 'petunia')]\n",
            "Novel Class Names: [(51, 'wild pansy'), (52, 'primula'), (53, 'sunflower'), (54, 'pelargonium'), (55, 'bishop of llandaff'), (56, 'gaura'), (57, 'geranium'), (58, 'orange dahlia'), (59, 'pink-yellow dahlia?'), (60, 'cautleya spicata'), (61, 'japanese anemone'), (62, 'black-eyed susan'), (63, 'silverbush'), (64, 'californian poppy'), (65, 'osteospermum'), (66, 'spring crocus'), (67, 'bearded iris'), (68, 'windflower'), (69, 'tree poppy'), (70, 'gazania'), (71, 'azalea'), (72, 'water lily'), (73, 'rose'), (74, 'thorn apple'), (75, 'morning glory'), (76, 'passion flower'), (77, 'lotus'), (78, 'toad lily'), (79, 'anthurium'), (80, 'frangipani'), (81, 'clematis'), (82, 'hibiscus'), (83, 'columbine'), (84, 'desert-rose'), (85, 'tree mallow'), (86, 'magnolia'), (87, 'cyclamen'), (88, 'watercress'), (89, 'canna lily'), (90, 'hippeastrum'), (91, 'bee balm'), (92, 'ball moss'), (93, 'foxglove'), (94, 'bougainvillea'), (95, 'camellia'), (96, 'mallow'), (97, 'mexican petunia'), (98, 'bromelia'), (99, 'blanket flower'), (100, 'trumpet creeper'), (101, 'blackberry lily')]\n"
          ]
        }
      ],
      "source": [
        "_, _, tmp_test = get_data()\n",
        "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
        "print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
        "print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8puO1VNpzwvi"
      },
      "source": [
        "## Split Dataset\n",
        "The next step is to actually split the dataset into the base and novel categories we extract from `base_novel_categories`.\n",
        "To split the data we need the dataset (obviously) and the list of base classes. If the sample label is not part of the base categories, then it must be part of the novel ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msOszMs2zRRu"
      },
      "outputs": [],
      "source": [
        "def split_data(dataset, base_classes):\n",
        "    # these two lists will store the sample indexes\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "\n",
        "    # we create a set of base classes to compute the test below in O(1)\n",
        "    # this is optional and can be removed\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    # here we iterate over sample labels and also get the correspondent sample index\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    # here we create the dataset subsets\n",
        "    # the torch Subset is just a wrapper around the dataset\n",
        "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
        "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
        "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "    return base_dataset, novel_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQZT22rE8hBw"
      },
      "source": [
        "## Extract k shots\n",
        "As the dataset already provides 10 train and validation shots, we do not need to extract them.\n",
        "Beaware that Few-Shot Adaptation papers must do this operation as most datasets count significantly more samples in both the training and validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KpbPRLr7WL_"
      },
      "source": [
        "## Load CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh6uLZRT7YJx",
        "outputId": "bf8875c0-f824-4b8d-f64c-e46538f2d9b5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:04<00:00, 71.7MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    <function _convert_image_to_rgb at 0x79d5a5987e20>\n",
              "    ToTensor()\n",
              "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "\n",
        "# preprocess contains CLIP's pre-defined augmentations, let's inspect them!\n",
        "preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM9H14899ses"
      },
      "source": [
        "## Load and Prepare Data\n",
        "Here we get the three dataset split and pass clip pre-defined augmentations.\n",
        "Then, we compute base and novel categories (in this case is redundand as we already did it before).\n",
        "Finally, se split the three datasets into base and novel categories.\n",
        "As we want to use the novel categories only for the test set, we drop `train_novel` and `val_novel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVrYUYTv9ttM"
      },
      "outputs": [],
      "source": [
        "# get the three datasets\n",
        "train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "\n",
        "# split classes into base and novel\n",
        "base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "# split the three datasets\n",
        "train_base, _ = split_data(train_set, base_classes)\n",
        "val_base, _ = split_data(val_set, base_classes)\n",
        "test_base, test_novel = split_data(test_set, base_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcgMwr3J9VIg"
      },
      "source": [
        "## Compute Zero-Shot Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uhblkvm9US4",
        "outputId": "bd545e96-8398-4234-9ff6-93d06ae246c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 20/20 [00:19<00:00,  1.03it/s]\n",
            "🧠 Zero-shot evaluation on Novel Classes: 100%|██████████| 29/29 [00:30<00:00,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Base classes accuracy: 71.33%\n",
            "🔍 Novel classes accuracy: 78.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad() # we don't want gradients\n",
        "def eval(model, dataset, categories, batch_size, device, label=\"\"):\n",
        "    # let's set the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Remap labels into a contiguous set starting from zero\n",
        "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
        "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
        "    text_inputs = clip.tokenize(\n",
        "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
        "    ).to(device)\n",
        "\n",
        "    # we can encode the text features once as they are shared for all images\n",
        "    # therefore we do it outside the evaluation loop\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "    # and here we normalize them (standard pratice with CLIP)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True) # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "    # simple dataloader creation\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "    for image, target in tqdm(dataloader, desc=label):\n",
        "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
        "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
        "        # Map targets in contiguous set starting from zero\n",
        "        # Labels needs to be .long() in pytorch\n",
        "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "        image = image.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # forward image through CLIP image encoder\n",
        "        image_features = model.encode_image(image)\n",
        "        # and normalize\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
        "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
        "        correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    # and now we compute the accuracy\n",
        "    accuracy = correct_predictions / len(dataset)\n",
        "    return accuracy\n",
        "\n",
        "base_accuracy = eval(model=model, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Base Classes\")\n",
        "novel_accuracy = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Novel Classes\")\n",
        "\n",
        "print()\n",
        "print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baYfLKNdfbUR"
      },
      "source": [
        "## Harmonic Mean\n",
        "Few-Shot Adaptations papers usually report the Harmonic Mean.\n",
        "The harmonic mean tends to mitigate the impact of large outliers (base accuracy) and aggravate the impact of small ones (novel accuracy).\n",
        "Thus, achieving very high base accuracies at the expense of the novel accuracy will be penalized by the HM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKAXR7hlfbUR",
        "outputId": "e00e50f4-3b0f-4e79-ed09-e8e82cc53668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Harmonic Mean: 74.62%\n"
          ]
        }
      ],
      "source": [
        "def harmonic_mean(base_accuracy, novel_accuracy):\n",
        "    numerator = 2\n",
        "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
        "    hm = numerator / denominator\n",
        "    return hm\n",
        "\n",
        "print(f\"🔍 Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "R08jwnaQ8R2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# Define data augmentation transformations\n",
        "augmentation_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),  # Random crop and resize\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # 50% chance to flip horizontally\n",
        "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color adjustments\n",
        "    transforms.RandomSolarize(0.5, p=1),\n",
        "    transforms.RandomRotation(degrees=15),  # Rotate image within ±15 degrees\n",
        "    #transforms.ToTensor(),  # Convert PIL image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize for CLIP\n",
        "])\n",
        "\n",
        "# Validation transformations (no augmentation, just normalization)\n",
        "validation_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to a fixed size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# Example dataset class\n",
        "class AugmentedImageTextDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data  # Assume data is a list of (image_path, label) tuples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "          image = self.transform(image)\n",
        "        return image, label\n",
        "augmented_train_base = AugmentedImageTextDataset(data = train_base, transform = augmentation_transforms)"
      ],
      "metadata": {
        "id": "6OA3pSfm8QFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning of the textual linear layer\n",
        "\n",
        "We fine-tune the last linear layer of the textual encoder for the classification of the base train data."
      ],
      "metadata": {
        "id": "eQPAGfZ9y-8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def fine_tuning_linear_text(model, train_dataset, val_dataset, categories, lr, batch_size, num_epochs, device):\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  model = model.float()\n",
        "\n",
        "  text_projection = model.text_projection\n",
        "\n",
        "  # Freeze all parameters in the model\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # Unfreeze the projection layer\n",
        "  text_projection.requires_grad = True\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = Adam([text_projection], lr=lr)\n",
        "\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "  print(\"🧠 Fine-tuning training+validation on Base Classes\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Training of one epoch\n",
        "\n",
        "    model.train()\n",
        "    # here we store the sum of all the computed losses through the all batches\n",
        "    total_loss = 0\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(train_dataloader):\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Training accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {total_loss/ len(train_dataloader)}; Training accuracy: {correct_predictions / len(train_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # Validation of one epoch\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(val_dataloader):\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Validation accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Validation loss: {total_loss/ len(val_dataloader)}; Validation accuracy: {correct_predictions / len(val_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "    print(\"-----------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return(model)\n",
        "\n",
        "model, _ = clip.load(\"ViT-B/16\", device=device)\n",
        "model_ft_text_layer = fine_tuning_linear_text(model=model, train_dataset=train_base, val_dataset=val_base, categories=base_classes, lr = 0.0001, batch_size=16, num_epochs=10, device=device)"
      ],
      "metadata": {
        "id": "vtwN7HP-w8of",
        "outputId": "f186ee50-8900-4e05-f08b-885b1c3e4c22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Fine-tuning training+validation on Base Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss: 3.7992719411849976; Training accuracy: 60.39%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation loss: 3.751607060432434; Validation accuracy: 76.27%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training loss: 3.7287665978074074; Training accuracy: 79.61%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Validation loss: 3.7079309299588203; Validation accuracy: 84.31%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training loss: 3.689993605017662; Training accuracy: 87.06%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Validation loss: 3.680222935974598; Validation accuracy: 88.43%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training loss: 3.6637882068753242; Training accuracy: 92.55%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Validation loss: 3.6623907387256622; Validation accuracy: 90.98%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Training loss: 3.6464011296629906; Training accuracy: 94.12%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Validation loss: 3.6505211740732193; Validation accuracy: 92.35%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Training loss: 3.6343393474817276; Training accuracy: 94.51%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Validation loss: 3.642144314944744; Validation accuracy: 91.96%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Training loss: 3.625513881444931; Training accuracy: 93.92%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Validation loss: 3.636342190206051; Validation accuracy: 94.12%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Training loss: 3.6201296970248222; Training accuracy: 96.67%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Validation loss: 3.631933718919754; Validation accuracy: 94.71%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Training loss: 3.6144131645560265; Training accuracy: 96.27%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Validation loss: 3.6280209496617317; Validation accuracy: 93.73%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Training loss: 3.6095264106988907; Training accuracy: 96.86%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Validation loss: 3.624827913939953; Validation accuracy: 94.31%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we evaluate the fine-tuned CLIP in both base test set (few-shot evaluation) and novel test set (zero-shot evaluation)."
      ],
      "metadata": {
        "id": "jnmc4AyAzMVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_accuracy = eval(model=model_ft_text_layer, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"🧠 Few-shot evaluation on Base Classes\")\n",
        "novel_accuracy = eval(model=model_ft_text_layer, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Novel Classes\")\n",
        "print()\n",
        "print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "h5podH53zHc7",
        "outputId": "d64f8cef-6aff-4581-c548-70a1eef37494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🧠 Few-shot evaluation on Base Classes: 100%|██████████| 20/20 [00:31<00:00,  1.58s/it]\n",
            "🧠 Zero-shot evaluation on Novel Classes: 100%|██████████| 29/29 [00:45<00:00,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Base classes accuracy: 92.72%\n",
            "🔍 Novel classes accuracy: 63.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning of the visual linear layer\n",
        "\n",
        "We fine-tune the last linear layer of the visual encoder for the classification of the base train data."
      ],
      "metadata": {
        "id": "RDz9XS0DhxnL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfzZ1Es-MwF5",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "9da4df7b-ed9c-448a-816f-52b9d087756e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "fine_tuning_linear_visual() got an unexpected keyword argument 'augmented_train_dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0e7c28c39bca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ViT-B/16\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mmodel_ft_visual_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tuning_linear_visual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmented_train_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: fine_tuning_linear_visual() got an unexpected keyword argument 'augmented_train_dataset'"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def fine_tuning_linear_visual(model, train_dataset, val_dataset, categories, lr, batch_size, num_epochs, device):\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  model = model.float()\n",
        "\n",
        "  visual_projection = model.visual.proj\n",
        "  text_projection = model.text_projection\n",
        "\n",
        "  # Freeze all parameters in the model\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # Unfreeze the projection layer\n",
        "  visual_projection.requires_grad = True\n",
        "  #text_projection.requires_grad = True\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = Adam([visual_projection], lr=lr)\n",
        "\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "\n",
        "\n",
        "  print(\"🧠 Fine-tuning training+validation on Base Classes\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Training of one epoch\n",
        "\n",
        "    model.train()\n",
        "    # here we store the sum of all the computed losses through the all batches\n",
        "    total_loss = 0\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(train_dataloader):\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Training accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {total_loss/ len(train_dataloader)}; Training accuracy: {correct_predictions / len(train_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # Validation of one epoch\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(val_dataloader):\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Validation accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Validation loss: {total_loss/ len(val_dataloader)}; Validation accuracy: {correct_predictions / len(val_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "    print(\"-----------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return(model)\n",
        "\n",
        "model, _ = clip.load(\"ViT-B/16\", device=device)\n",
        "model_ft_visual_layer = fine_tuning_linear_visual(model=model, train_dataset=augmented_train_base, val_dataset=val_base, categories=base_classes, lr = 0.0001, batch_size=16, num_epochs=30, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we evaluate the fine-tuned CLIP in both base test set (few-shot evaluation) and novel test set (zero-shot evaluation)."
      ],
      "metadata": {
        "id": "xzrUWPcwu00p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_accuracy = eval(model=model_ft_visual_layer, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"🧠 Few-shot evaluation on Base Classes\")\n",
        "novel_accuracy = eval(model=model_ft_visual_layer, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Novel Classes\")\n",
        "print()\n",
        "print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "h1Xyn8_hldt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b32264-8156-43c1-cb11-eece01111142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🧠 Few-shot evaluation on Base Classes: 100%|██████████| 20/20 [00:31<00:00,  1.59s/it]\n",
            "🧠 Zero-shot evaluation on Novel Classes: 100%|██████████| 29/29 [00:46<00:00,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Base classes accuracy: 82.81%\n",
            "🔍 Novel classes accuracy: 52.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using CLIP original loss"
      ],
      "metadata": {
        "id": "mY4XTSFHxDyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "temperature = 0.07\n",
        "\n",
        "def fine_tuning_linear_visual(model, train_dataset, val_dataset, categories, lr, batch_size, num_epochs, device):\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  model = model.float()\n",
        "\n",
        "  visual_projection = model.visual.proj\n",
        "  text_projection = model.text_projection\n",
        "\n",
        "  # Freeze all parameters in the model\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # Unfreeze the projection layer\n",
        "  visual_projection.requires_grad = True\n",
        "  #text_projection.requires_grad = True\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = Adam([visual_projection], lr=lr)\n",
        "\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "\n",
        "\n",
        "  print(\"🧠 Fine-tuning training+validation on Base Classes\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Training of one epoch\n",
        "\n",
        "    model.train()\n",
        "    # here we store the sum of all the computed losses through the all batches\n",
        "    total_loss = 0\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(train_dataloader):\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in target]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "          target = torch.argsort(torch.argsort(target))#create a vector that has integers betwee 0 and batch_size\n",
        "          target = torch.arange(len(target))\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = (image_features @ text_features.T)* torch.exp(torch.tensor([temperature]).to(device))\n",
        "\n",
        "          loss = (criterion(logits, target) +criterion(logits.T, target))/2\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Training accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {total_loss/ len(train_dataloader)}; Training accuracy: {correct_predictions / len(train_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # Validation of one epoch\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(val_dataloader):\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Validation accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Validation loss: {total_loss/ len(val_dataloader)}; Validation accuracy: {correct_predictions / len(val_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "    print(\"-----------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return(model)\n",
        "\n",
        "model, _ = clip.load(\"ViT-B/16\", device=device)\n",
        "model_ft_visual_layer = fine_tuning_linear_visual(model=model, train_dataset=train_base, val_dataset=val_base, categories=base_classes, lr = 0.0001, batch_size=16, num_epochs=30, device=device)\n"
      ],
      "metadata": {
        "id": "ZKAQ6HtQxBkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}